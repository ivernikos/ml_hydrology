{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48474472",
   "metadata": {},
   "source": [
    "## Import nessecary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e44640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics import F1Score\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import Precision\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join, exists\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce3e05",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_labels(y):\n",
    "    y = torch.where(y == 1 , 1, y)\n",
    "    y = torch.where(y == 0, 0, y)\n",
    "    y = torch.where(y == -1, 0, y)\n",
    "\n",
    "    return y.squeeze()\n",
    "\n",
    "\n",
    "def open_raster(path):\n",
    "    raster = rasterio.open(path)\n",
    "    if raster == None: print('Error opening file: {}'.format(path))\n",
    "    band_ls = []\n",
    "  \n",
    "    #for every band\n",
    "    b=1\n",
    "    while b<=raster.count:\n",
    "        band = raster.read(b) #open band and save it as a numpy array\n",
    "        band_ls.append(band) #append band to the list\n",
    "        b = b+1\n",
    "  \n",
    "    image = np.stack(band_ls)\n",
    "    raster.close()\n",
    "    return image\n",
    "\n",
    "def save_to_raster(x, src_data_path,  path):\n",
    "    '''Save a numpy array(x) to a specified path (path) as a raster file \n",
    "      with the same projection and geotransform as a given raster file(src_data)'''\n",
    "    \n",
    "    src_data = rasterio.open(src_data_path)\n",
    "    x = np.squeeze(x)\n",
    "    with rasterio.open(\n",
    "    path,\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=x.shape[1],\n",
    "    width=x.shape[2],\n",
    "    count = x.shape[0],\n",
    "    dtype=x.dtype,\n",
    "    crs= src_data.crs,\n",
    "    transform=src_data.transform,\n",
    "    ) as dst:\n",
    "        dst.write(x)\n",
    "    dst.close()\n",
    "    \n",
    "#Normalize the data values between 0 and 1\n",
    "def normalize(image):\n",
    "    \n",
    "    i=0\n",
    "    std_ls=[]\n",
    "    mean_ls=[]\n",
    "\n",
    "    while i<image.shape[0]:\n",
    "        mean = image[i].mean()\n",
    "        std = image[i].std()\n",
    "        if std == 0:\n",
    "            std = std + 0.1\n",
    "        mean_ls.append(mean)\n",
    "        std_ls.append(std)\n",
    "        i= i+1\n",
    "\n",
    "    mean_ls = torch.from_numpy(np.array(mean_ls).reshape(image.shape[0],1,1))\n",
    "    std_ls = torch.from_numpy(np.array(std_ls).reshape(image.shape[0],1,1))\n",
    "  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Normalize(mean_ls, std_ls)\n",
    "    ]) \n",
    "\n",
    "    image = torch.from_numpy(image).float()\n",
    "    image = transform(image)\n",
    "    with torch.no_grad():\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        image = sigmoid(image)\n",
    "    return image\n",
    "\n",
    "#Apply transforms to the data and labels\n",
    "def apply_transforms(image, mode):\n",
    "\n",
    "    if mode == 'image':\n",
    "        transforms_img = transforms.Compose([\n",
    "            transforms.Lambda(normalize)\n",
    "        ])\n",
    "        \n",
    "        result = transforms_img(image)\n",
    "    \n",
    "    if mode == 'label':\n",
    "        transforms_label = transforms.Compose([\n",
    "            transforms.Lambda(change_labels)\n",
    "        ])\n",
    "\n",
    "        result = transforms_label(image)\n",
    "\n",
    "    return result\n",
    "\n",
    "def calc_entropy(prediction): \n",
    "    '''Calculate Shannon's entropy. The formula is:\n",
    "            H(X) = −Σ P(xi) log2 (P(xi)) '''\n",
    "    \n",
    "    entropy = prediction * torch.log2(prediction)\n",
    "    entropy = torch.sum(entropy, dim = 0)\n",
    "    entropy = torch.mean(entropy).item()\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def maxelements(df, N):\n",
    "    '''Get the N biggest numbers contained in a given dataframe's column '''\n",
    "    \n",
    "    df_max = pd.DataFrame([], columns = ['Entropy', 'Filenames'])\n",
    "    \n",
    "    for i in range(0, N):\n",
    "        idxmax = df['Entropy'].idxmax()\n",
    "        df_max = pd.concat([df_max, df.iloc[[idxmax]]])\n",
    "        df = df.drop(idxmax)\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    return df_max\n",
    "\n",
    "def extract_highest_entropy(unlabeled_df, path_unlabeled, N):\n",
    "    entropy_ls = []\n",
    "    filenames_ls = []\n",
    "    files = len(unlabeled_df)\n",
    "    i = 1\n",
    "    \n",
    "    for f in listdir(path_unlabeled):\n",
    "        if unlabeled_df['Filenames'].str.contains(f).any():\n",
    "            print('Calculating entropy...Progress {}/{}'.format(i, files), end=\"\\r\", flush=True)\n",
    "            with torch.no_grad():\n",
    "                X = open_raster(join(path_unlabeled, f))\n",
    "                X = X.astype('float')\n",
    "                X = apply_transforms(X, mode = 'image')\n",
    "                X = X.to(device)\n",
    "                X = X[None, :, :]\n",
    "                pred = model(X)\n",
    "                entropy = calc_entropy(pred)\n",
    "                entropy_ls.append(entropy)\n",
    "                filenames_ls.append(f)\n",
    "                i += 1\n",
    "    \n",
    "    df = pd.DataFrame((list(zip(filenames_ls, entropy_ls))), columns =['Filenames', 'Entropy'])\n",
    "    max_entropy = maxelements(df, N)\n",
    "    print('')\n",
    "    \n",
    "    return max_entropy\n",
    "\n",
    "def create_labels(max_entropy, model, path_unlabeled, new_labels_dir):\n",
    "    print('Creating new labels...')\n",
    "    files = len(max_entropy)\n",
    "    i = 1\n",
    "    for f in listdir(path_unlabeled):\n",
    "        if max_entropy['Filenames'].str.contains(f).any():\n",
    "            print('File: {}/{}'.format(i, files), end = '\\r', flush = True)\n",
    "            with torch.no_grad():\n",
    "                X = open_raster(join(path_unlabeled, f))\n",
    "                X = X.astype('float')\n",
    "                X = apply_transforms(X, mode = 'image')\n",
    "                X = X.to(device)\n",
    "                X = X[None, :, :]\n",
    "                pred = model(X)\n",
    "                pred = torch.where(pred <= 0.5 , 0, pred)\n",
    "                pred = torch.where(pred > 0.5 , 1, pred)\n",
    "                pred = pred.cpu().numpy()\n",
    "                src = join(path_unlabeled, f)\n",
    "                save_path = join(new_labels_dir, f)\n",
    "                save_to_raster(pred, src, save_path)\n",
    "                i = i + 1\n",
    "    print('')\n",
    "                \n",
    "def delete_dir(dirpath):\n",
    "    print('Deleting old labels...')\n",
    "    for filename in os.listdir(dirpath):\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        try:\n",
    "            shutil.rmtree(filepath)\n",
    "        except OSError:\n",
    "            os.remove(filepath)\n",
    "\n",
    "def visualise_training(H, A, save_path, i):\n",
    "    f, axarr = plt.subplots(2,1)\n",
    "    f.set_figheight(10)\n",
    "    f.set_figwidth(10)\n",
    "    axarr[0].set_title(\"Loss\")\n",
    "    axarr[1].set_title(\"Acurracy\")\n",
    "\n",
    "    axarr[0].plot(H[\"train_loss\"], label=\"train loss\")\n",
    "    axarr[0].plot(H[\"valid_loss\"], label=\"valid loss\")\n",
    "    axarr[1].plot(A[\"train_accuracy\"], label=\"training accuracy\")\n",
    "    axarr[1].plot(A[\"valid_accuracy\"], label=\"validation accuracy\")\n",
    "\n",
    "    #Save plot\n",
    "    plt.savefig(join(save_path, \"training/training{}.png\".format(i + 1)))\n",
    "\n",
    "def visualise_evaluation(pred_ls, y_ls):\n",
    "    counter = 0\n",
    "    fig, axs = plt.subplots(nrows=15, ncols=2,figsize=(10,50))\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    for b in range(15):\n",
    "        \n",
    "        axs[counter, 0].imshow(pred_ls[b].to('cpu').squeeze(), cmap = 'gray')\n",
    "        axs[counter, 1].imshow(y_ls[b].to('cpu').squeeze(), cmap = 'gray')\n",
    "        counter += 1\n",
    "        \n",
    "    fig.tight_layout(pad=0)   \n",
    "    \n",
    "def save_model(save_path, i):\n",
    "    torch.save(model.state_dict(), join(save_path, 'models/model{}.pt'.format(i+1)))\n",
    "    torch.save(optimizer.state_dict(), join(save_path,\"optimizers/optimizer{}.pt\".format(i+1)))\n",
    "\n",
    "def update_pools(labeled_pool, unlabeled_pool, max_entropy):\n",
    "    print('Updating pools...')\n",
    "    max_entropy = max_entropy.drop(['Entropy'], axis=1)\n",
    "    labeled_pool = pd.concat([labeled_pool, max_entropy], ignore_index=True)\n",
    "    unlabeled_pool = unlabeled_pool[~unlabeled_pool.Filenames.isin(max_entropy['Filenames'])]\n",
    "    print('Updating pools...Done', end = \"\\r\", flush = True)\n",
    "    return labeled_pool, unlabeled_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3391c",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978dc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labeled(TensorDataset):\n",
    "    def __init__(self, csv_path, images_path, labels_path, preprocess = True):\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.csv_path = csv_path\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "\t\t# return the number of total samples contained in the dataset\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\t\t# grab the image and label from the current index\n",
    "        filename = self.df['Filenames'][i]\n",
    "        image_path = self.images_path + \"/\" + filename\n",
    "        label_path = self.labels_path + \"/\" + filename\n",
    "        \n",
    "        image = open_raster(image_path)\n",
    "        label = open_raster(label_path)\n",
    "        image = image.astype('float')\n",
    "        label = torch.from_numpy(label)\n",
    "        label = label[None, :, :].long()\n",
    "        \n",
    "\t\t# apply the transformations to image\n",
    "        if self.preprocess:\n",
    "            image = apply_transforms(image, mode = 'image')\n",
    "            label = apply_transforms(label, mode = 'label')\n",
    "\n",
    "\t\t# return a tuple of the image and its mask\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unlabeled(TensorDataset):\n",
    "    def __init__(self, labeled_pool, labeled_dir, labels_dir, unlabeled_dir,new_labels_dir, model):\n",
    "        self.labeled_pool = labeled_pool\n",
    "        self.labeled_dir = labeled_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.unlabeled_dir = unlabeled_dir\n",
    "        self.new_labels_dir = new_labels_dir\n",
    "        self.model = model\n",
    "        \n",
    "    def __len__(self):\n",
    "\t\t# return the number of total samples contained in the dataset\n",
    "        return len(self.labeled_pool)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\t\t# grab the image and label from the current index\n",
    "        filename = self.labeled_pool['Filenames'][i]\n",
    "        \n",
    "        image_path = self.labeled_dir + \"/\" + filename\n",
    "        if os.path.exists(image_path):\n",
    "            image_path = self.labeled_dir + \"/\" + filename\n",
    "            label_path = self.labels_dir + \"/\" + filename\n",
    "        else:\n",
    "            image_path = self.unlabeled_dir + \"/\" + filename\n",
    "            label_path = self.new_labels_dir + \"/\" + filename\n",
    "        \n",
    "        image = open_raster(image_path)\n",
    "        label = open_raster(label_path)\n",
    "        image = image.astype('float')\n",
    "        label = torch.from_numpy(label)\n",
    "        label = label[None, :, :].long()\n",
    "        \n",
    "\t\t# apply the transformations to image\n",
    "        image = apply_transforms(image, mode = 'image')\n",
    "        label = apply_transforms(label, mode = 'label')\n",
    "\t\t# return a tuple of the image and its mask\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785712c0",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv =  nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3, \n",
    "                      stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up_conv = nn.Sequential( \n",
    "            nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                              kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "          )\n",
    "  \n",
    "    def forward(self,x):\n",
    "        return self.up_conv(x)\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv =  nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3, \n",
    "                      stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=3, \n",
    "                      stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff75eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = double_conv(13, 64)\n",
    "        self.conv2 = double_conv(64, 128)\n",
    "        self.conv3 = double_conv(128, 256)\n",
    "        self.conv4 = double_conv(256, 512)\n",
    "        self.conv5 = double_conv(512, 1024)\n",
    "        self.up_conv6 = up_conv(1024, 512)\n",
    "        self.conv6 = conv(1024, 512)\n",
    "        self.up_conv7 = up_conv(512, 256)\n",
    "        self.conv7 = conv(512, 256)\n",
    "        self.up_conv8 = up_conv(256, 128)\n",
    "        self.conv8 = conv(256, 128)\n",
    "        self.up_conv9 = up_conv(128, 64)\n",
    "        self.conv9 = conv(128, 64)\n",
    "        self.conv10 = nn.Sequential(\n",
    "            nn.Conv2d(64, 2, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        block1 = self.conv1(x)\n",
    "        x = F.max_pool2d(block1, kernel_size=2)\n",
    "        block2 = self.conv2(x)\n",
    "        x = F.max_pool2d(block2,kernel_size=2)\n",
    "        block3 = self.conv3(x)\n",
    "        x = F.max_pool2d(block3,kernel_size=2)\n",
    "        block4 = self.conv4(x)\n",
    "        x = F.max_pool2d(block4,kernel_size=2)\n",
    "        x = self.conv5(x)\n",
    "        x = self.up_conv6(x)\n",
    "        x = torch.cat([x, block4], dim=1)\n",
    "        x = self.conv6(x)\n",
    "        x = self.up_conv7(x)\n",
    "        x = torch.cat([x, block3], dim=1)\n",
    "        x = self.conv7(x)\n",
    "        x = self.up_conv8(x)\n",
    "        x = torch.cat([x, block2], dim=1)\n",
    "        x = self.conv8(x)\n",
    "        x = self.up_conv9(x)\n",
    "        x = torch.cat([x, block1], dim=1)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5861132",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, valid_loader):\n",
    "    print('\\033[1mTraining model...\\033[0m')\n",
    "    print('\\033[1m------------------------\\033[0m')\n",
    "    accuracy = Accuracy().to(device)\n",
    "\n",
    "    training_size = len(train_loader.dataset)\n",
    "    valid_size = len(valid_loader.dataset)\n",
    "    train_steps = len(train_loader)\n",
    "    valid_steps = len(valid_loader)\n",
    "\n",
    "    H = {\"train_loss\": [], \"valid_loss\": []}\n",
    "    A = {\"train_accuracy\": [], \"valid_accuracy\": []}\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(epochs): #For every epoch\n",
    "        totalTrainLoss = 0\n",
    "        totalValidLoss = 0\n",
    "        train_accuracy = 0\n",
    "        valid_accuracy = 0\n",
    "    \n",
    "        for batch, (X, y) in enumerate(train_loader): #for every batch (the dataloader calls it)\n",
    "        \n",
    "            X, y = X.to(device), y.to(device) #load the batch to the desired device\n",
    "            pred = model(X) #forward pass\n",
    "            # Compute prediction error\n",
    "            loss = criterion(pred, y)\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            accur = accuracy(pred.view(-1), y.view(-1)).item()\n",
    "            train_accuracy += float(accur)\n",
    "            totalTrainLoss += float(loss)\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad() #zero the gradients so they don't stack up\n",
    "            loss.backward() #back-propagation\n",
    "            optimizer.step() #update model parameters\n",
    "            \n",
    "        with torch.no_grad(): #Disable gradient tracking\n",
    "            for X, y in valid_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = criterion(pred, y)\n",
    "                \n",
    "                pred = torch.argmax(pred, dim=1)\n",
    "                accur = accuracy(pred.view(-1), y.view(-1)).item()\n",
    "                valid_accuracy += float(accur)\n",
    "                totalValidLoss += float(loss)\n",
    "\n",
    "        avgTrainLoss = totalTrainLoss / (train_steps)\n",
    "        avgValidLoss = totalValidLoss / (valid_steps)\n",
    "        avg_train_accuracy = train_accuracy / train_steps\n",
    "        avg_valid_accuracy = valid_accuracy / valid_steps\n",
    "    \n",
    "        H[\"train_loss\"].append(avgTrainLoss)\n",
    "        H[\"valid_loss\"].append(avgValidLoss)\n",
    "        A[\"train_accuracy\"].append(avg_train_accuracy)\n",
    "        A[\"valid_accuracy\"].append(avg_valid_accuracy)\n",
    "\n",
    "        print(\"\\033[1mEpoch\\033[0m: {}/{}...Train loss: {:.6f}, Valid loss: {:.4f}, Train accuracy: {:.2f}, Valid accuracy: {:.2f}\".format(\n",
    "            i + 1, epochs,avgTrainLoss, avgValidLoss,avg_train_accuracy, avg_valid_accuracy), end=\"\\r\", flush=True)\n",
    "    \n",
    "    print()\n",
    "    print(\"Training done!\")\n",
    "    return H,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(save_path, i):\n",
    "    print()\n",
    "    print('\\033[1mEvaluating model...\\033[0m')\n",
    "    print('\\033[1m-------------------------\\033[0m')\n",
    "    model.eval()\n",
    "\n",
    "    #Create empty lists to save metrics\n",
    "    accuracy_ls = []\n",
    "    precision_ls = []\n",
    "    jaccard_ls = []\n",
    "    jaccard_sp = 0\n",
    "    f1_ls = []\n",
    "    pred_ls = []\n",
    "    y_ls = []\n",
    "    confmat_total = torch.zeros((2,2)).to(device)\n",
    "\n",
    "    #Begin prediction\n",
    "    with torch.no_grad(): #Disable gradient tracking\n",
    "        for X, y in eval_loader:\n",
    "            X, y = X.to(device), y.to(device) \n",
    "\n",
    "            pred = model(X) #predict from data\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            pred_ls.append(pred)\n",
    "         \n",
    "            y = y.int()\n",
    "            y_ls.append(y)\n",
    "            y = torch.where(y == -1, 0, y)\n",
    "\n",
    "            confmat = ConfusionMatrix(num_classes=2, is_multilabel = True).to(device)\n",
    "            metric = confmat(pred.view(-1), y.view(-1))\n",
    "            confmat_total = confmat_total + metric\n",
    "\n",
    "            precision = Precision().to(device)\n",
    "            metric = precision(pred.view(-1), y.view(-1)).item()\n",
    "            precision_ls.append(metric)\n",
    "\n",
    "            accuracy = Accuracy().to(device)\n",
    "            metric = accuracy(pred.view(-1), y.view(-1)).item()\n",
    "            accuracy_ls.append(metric)\n",
    "\n",
    "            jaccard = JaccardIndex(num_classes=2).to(device)\n",
    "            metric = jaccard(pred, y).item()\n",
    "            jaccard_ls.append(metric)\n",
    "        \n",
    "            jaccard2 = JaccardIndex(num_classes=2, average = 'none').to(device)\n",
    "            metric = jaccard2(pred, y)\n",
    "            jaccard_sp += metric \n",
    "\n",
    "            f1 = F1Score(num_classes=2).to(device)\n",
    "            metric = f1(pred.view(-1), y.view(-1)).item()\n",
    "            f1_ls.append(metric)\n",
    "\n",
    "    jaccard_av = sum(jaccard_ls) / len(jaccard_ls)\n",
    "    jaccard_sp_av = jaccard_sp / len(eval_loader)\n",
    "    f1_av = sum(f1_ls) / len(f1_ls)\n",
    "    accuracy_av = sum(accuracy_ls) / len(accuracy_ls)\n",
    "    precision_av = sum(precision_ls) / len(precision_ls)\n",
    "    \n",
    "    print(\"Average IoU (Intercection over Union): {} \\nAverage F1 Score: {}\".format(jaccard_av, f1_av))\n",
    "    print()\n",
    "    print(\"Average IoU for every catecory:\")\n",
    "    print(\"No flood:\", jaccard_sp_av[0])\n",
    "    print(\"Flood:\", jaccard_sp_av[1])\n",
    "    print()\n",
    "    print(\"Average accuracy: {} \\nAverage precision: {}\".format(accuracy_av, precision_av))\n",
    "    print()\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confmat_total)\n",
    "    \n",
    "    save_path = join(save_path,\"evaluations/evaluation{}.txt\".format(i+1))\n",
    "    f = open(save_path, \"w\")\n",
    "    f.write(\"Epochs:{}\\nBatch:{}\\nOptimizer:{}\\nLoss function:{}\".format(epochs, train_loader.batch_size,optimizer ,'CrossEntropyLoss' ))\n",
    "    f.write(\"\\nAverage IoU (Intercection over Union): {} \\nAverage F1 Score: {}\".format(jaccard_av, f1_av))\n",
    "    f.write('')\n",
    "    f.write(\"Average IoU for every category:\")\n",
    "    f.write(\"No flood:{}\".format(jaccard_sp_av[0]))\n",
    "    f.write(\"Flood:{}\".format(jaccard_sp_av[1]))\n",
    "    f.write('')\n",
    "    f.write(\"\\nAverage accuracy: {} \\nAverage precision: {}\".format(accuracy_av, precision_av))\n",
    "    f.write(\"\\nConfusion matrix:\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    with open(save_path, \"a\") as csvfile:\n",
    "        np.savetxt(csvfile, confmat_total.cpu().numpy(), fmt='%s')\n",
    "\n",
    "    return pred_ls, y_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2f6da",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Parameters\n",
    "labeled_dir = ''\n",
    "labels_dir = ''\n",
    "evaluation_dir = ''\n",
    "evaluation_labels_dir = ''\n",
    "unlabeled_dir = ''\n",
    "new_labels_dir = ''\n",
    "csv_train = ''\n",
    "csv_valid = ''\n",
    "csv_eval = ''\n",
    "csv_unlabeled = ''\n",
    "save_path = ''\n",
    "\n",
    "torch.manual_seed(20) \n",
    "if torch.cuda.is_available(): device = 'cuda' \n",
    "else: device = 'cpu' \n",
    "model = UNet().to(device) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 1e-6)\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "N = 100\n",
    "training_cycles = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets and Dataloaders\n",
    "labeled_pool = pd.read_csv(csv_train)\n",
    "unlabeled_pool = pd.read_csv(csv_unlabeled)\n",
    "\n",
    "train_dataset = Labeled(csv_train, labeled_dir, labels_dir)\n",
    "valid_dataset = Labeled(csv_valid, labeled_dir, labels_dir)\n",
    "eval_dataset = Labeled(csv_eval, evaluation_dir, evaluation_labels_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle = True, pin_memory = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size, shuffle = True, pin_memory = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = 1, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa26d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "start_time = time.time()#time the training starts\n",
    "i = -1\n",
    "H, A = train_model(train_loader, valid_loader)\n",
    "visualise_training(H, A, save_path, i)\n",
    "save_model(save_path, i)\n",
    "pred_ls, y_ls = eval_model(save_path, i)\n",
    "visualise_evaluation(pred_ls, y_ls)\n",
    "pred_ls = []\n",
    "y_ls =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (training_cycles):\n",
    "    print('\\033[1mTraining cycle:\\033[0m', i+1)\n",
    "    print('\\033[1m-----------------------\\033[0m')\n",
    "    max_entropy = extract_highest_entropy(unlabeled_pool, unlabeled_dir, N)\n",
    "    create_labels(max_entropy, model, unlabeled_dir, new_labels_dir)\n",
    "    labeled_pool, unlabeled_pool = update_pools(labeled_pool, unlabeled_pool, max_entropy)\n",
    "    unlabeled_dataset = Unlabeled(labeled_pool, labeled_dir, labels_dir, unlabeled_dir,new_labels_dir, model)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size, shuffle = True, pin_memory = True)\n",
    "    \n",
    "    epochs += 0\n",
    "    model = UNet().to(device) \n",
    "    H, A = train_model(unlabeled_loader, valid_loader)\n",
    "    visualise_training(H, A, save_path, i+1)\n",
    "    save_model(save_path, i+1)\n",
    "    if i%5 == 0:\n",
    "        pred_ls, y_ls = eval_model(save_path, i+1)\n",
    "        pred_ls = []\n",
    "        y_ls = []\n",
    "    delete_dir(new_labels_dir)\n",
    "    create_labels(unlabeled_pool, model, unlabeled_dir, new_labels_dir)\n",
    "\n",
    "visualise_evaluation(pred_ls, y_ls)\n",
    "print('Training complete!!')\n",
    "print(f'\\nDuration: {(time.time() - start_time)/60:.0f} minutes') #print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a826ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
